{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPJ6qP2GjxbFcSWFiMeJCjT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hadil-sgh/-Machine-Learning-Projects/blob/main/Speaker_Diarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "HZX5iLN6_HdE",
        "outputId": "4d3f4d87-c83d-4e9c-adfe-bdfb4bca2510"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'sounddevice'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e785a5e0f3b4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msounddevice\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfaster_whisper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWhisperModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sounddevice'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import os\n",
        "import sounddevice as sd\n",
        "import numpy as np\n",
        "import torch\n",
        "from faster_whisper import WhisperModel\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from pynput import keyboard\n",
        "import time\n",
        "import signal\n",
        "import threading\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "import tempfile\n",
        "import wave\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "os.environ[\"HF_HUB_DISABLE_SYMLINKS\"] = \"1\"\n",
        "\n",
        "# Audio settings\n",
        "SAMPLE_RATE = 16000\n",
        "CHANNELS = 1\n",
        "DEVICE_INDEX = 1\n",
        "MIN_RECORD_TIME = 1.0   # seconds\n",
        "SEGMENT_DURATION = 2.0  # seconds\n",
        "STEP_DURATION = 1.0     # seconds\n",
        "\n",
        "# Model settings\n",
        "WHISPER_MODEL_SIZE = \"tiny\"\n",
        "DEVICE = \"cpu\"\n",
        "\n",
        "# Globals\n",
        "audio_buffer = []\n",
        "RECORDING = False\n",
        "recording_start_time = None\n",
        "processing_thread = None\n",
        "exit_flag = False\n",
        "\n",
        "print(\"Loading models...\")\n",
        "\n",
        "# 1. Initialize Whisper for transcription\n",
        "whisper_model = WhisperModel(WHISPER_MODEL_SIZE, device=DEVICE)\n",
        "print(\"‚úÖ Whisper model loaded\")\n",
        "\n",
        "# 2. Initialize PyAnnote for speaker diarization\n",
        "try:\n",
        "    from pyannote.audio import Pipeline\n",
        "    from pyannote.audio.pipelines.utils.hook import ProgressHook\n",
        "\n",
        "    # Use a temporary directory to avoid permission issues\n",
        "    temp_dir = tempfile.mkdtemp()\n",
        "    os.environ[\"HF_HOME\"] = temp_dir\n",
        "\n",
        "    # Initialize the pipeline\n",
        "    diarization_pipeline = Pipeline.from_pretrained(\n",
        "        \"pyannote/speaker-diarization-3.1\",\n",
        "        use_auth_token=False  # Set to your HF token if needed\n",
        "    )\n",
        "\n",
        "    # Move to CPU\n",
        "    diarization_pipeline.to(DEVICE)\n",
        "    print(\"‚úÖ PyAnnote diarization model loaded\")\n",
        "    USE_PYANNOTE = True\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Could not load PyAnnote: {e}\")\n",
        "    print(\"‚ö†Ô∏è Falling back to basic diarization\")\n",
        "    USE_PYANNOTE = False\n",
        "\n",
        "    # Try to load SpeechBrain as fallback\n",
        "    try:\n",
        "        from speechbrain.inference import EncoderClassifier\n",
        "\n",
        "        # Try to load the model without symlinks\n",
        "        speaker_model = EncoderClassifier.from_hparams(\n",
        "            source=\"speechbrain/spkrec-ecapa-voxceleb\",\n",
        "            run_opts={\"device\": DEVICE}\n",
        "        )\n",
        "        print(\"‚úÖ SpeechBrain model loaded as fallback\")\n",
        "        USE_SPEECHBRAIN = True\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Could not load SpeechBrain: {e}\")\n",
        "        USE_SPEECHBRAIN = False\n",
        "\n",
        "# Check audio device\n",
        "try:\n",
        "    device_info = sd.query_devices(DEVICE_INDEX)\n",
        "    if device_info['max_input_channels'] < CHANNELS:\n",
        "        raise ValueError(\"Selected device does not support enough input channels.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Error: {e}\")\n",
        "    print(\"üîç Searching for a valid input device...\")\n",
        "    for i, d in enumerate(sd.query_devices()):\n",
        "        if d['max_input_channels'] >= CHANNELS:\n",
        "            DEVICE_INDEX = i\n",
        "            print(f\"‚úÖ Auto-selected input device #{i}: {d['name']}\")\n",
        "            break\n",
        "    else:\n",
        "        raise RuntimeError(\"‚ùå No suitable input device found.\")\n",
        "\n",
        "# Audio callback\n",
        "def callback(indata, frames, time, status):\n",
        "    global RECORDING, audio_buffer\n",
        "    if status:\n",
        "        print(f\"‚ö†Ô∏è Audio callback status: {status}\")\n",
        "    if RECORDING:\n",
        "        audio_buffer.append(indata.copy())\n",
        "\n",
        "# Save audio to WAV file\n",
        "def save_audio_to_file(audio_data, filename=\"temp_audio.wav\"):\n",
        "    \"\"\"Save audio data to a WAV file\"\"\"\n",
        "    with wave.open(filename, 'wb') as wf:\n",
        "        wf.setnchannels(CHANNELS)\n",
        "        wf.setsampwidth(2)  # 16-bit\n",
        "        wf.setframerate(SAMPLE_RATE)\n",
        "        # Convert float32 to int16\n",
        "        audio_int = (audio_data * 32767).astype(np.int16)\n",
        "        wf.writeframes(audio_int.tobytes())\n",
        "    return filename\n",
        "\n",
        "# Extract embeddings using SpeechBrain\n",
        "def extract_embeddings(audio_segments):\n",
        "    \"\"\"Extract speaker embeddings using SpeechBrain\"\"\"\n",
        "    if not USE_SPEECHBRAIN:\n",
        "        return None\n",
        "\n",
        "    embeddings = []\n",
        "    for _, _, segment_audio in audio_segments:\n",
        "        tensor = torch.from_numpy(segment_audio).float().unsqueeze(0)\n",
        "        emb = speaker_model.encode_batch(tensor).squeeze().detach().cpu().numpy()\n",
        "        embeddings.append(emb)\n",
        "\n",
        "    return np.vstack(embeddings) if embeddings else None\n",
        "\n",
        "# Process audio in a separate thread\n",
        "def process_audio_thread(audio_data):\n",
        "    print(\"üéôÔ∏è Processing audio...\")\n",
        "\n",
        "    try:\n",
        "        # Normalize audio\n",
        "        audio_data = audio_data / np.max(np.abs(audio_data)) if np.max(np.abs(audio_data)) > 0 else audio_data\n",
        "        print(f\"üîç Audio data length: {len(audio_data)}\")\n",
        "\n",
        "        # 1. SEGMENT THE AUDIO\n",
        "        num_samples = len(audio_data)\n",
        "        segment_samples = int(SAMPLE_RATE * SEGMENT_DURATION)\n",
        "        step_samples = int(SAMPLE_RATE * STEP_DURATION)\n",
        "\n",
        "        segments = []\n",
        "        for start in range(0, num_samples - segment_samples + 1, step_samples):\n",
        "            end = start + segment_samples\n",
        "            segments.append((start, end, audio_data[start:end]))\n",
        "\n",
        "        print(f\"üìù {len(segments)} audio segments generated\")\n",
        "\n",
        "        # 2. SPEAKER DIARIZATION\n",
        "        if USE_PYANNOTE and len(audio_data) > SAMPLE_RATE:  # At least 1 second of audio\n",
        "            # Save audio to file for PyAnnote\n",
        "            audio_file = save_audio_to_file(audio_data)\n",
        "            print(f\"Audio saved to {audio_file}\")\n",
        "\n",
        "            # Run diarization\n",
        "            print(\"Running PyAnnote diarization...\")\n",
        "            with ProgressHook() as hook:\n",
        "                diarization = diarization_pipeline(audio_file, hook=hook)\n",
        "\n",
        "            # Extract speaker turns\n",
        "            speaker_turns = []\n",
        "            for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
        "                speaker_turns.append({\n",
        "                    'start': turn.start,\n",
        "                    'end': turn.end,\n",
        "                    'speaker': speaker\n",
        "                })\n",
        "\n",
        "            print(f\"üîñ {len(set(turn['speaker'] for turn in speaker_turns))} speakers detected\")\n",
        "\n",
        "            # 3. TRANSCRIBE AND ASSIGN SPEAKERS\n",
        "            print(\"Transcribing with speaker labels...\")\n",
        "\n",
        "            # Process each speaker turn\n",
        "            for turn in speaker_turns:\n",
        "                # Convert time to samples\n",
        "                start_sample = int(turn['start'] * SAMPLE_RATE)\n",
        "                end_sample = min(int(turn['end'] * SAMPLE_RATE), len(audio_data))\n",
        "\n",
        "                # Extract audio segment\n",
        "                if end_sample > start_sample:\n",
        "                    segment_audio = audio_data[start_sample:end_sample]\n",
        "\n",
        "                    # Transcribe segment\n",
        "                    segment_audio = segment_audio.astype(np.float32)\n",
        "                    segments_result, _ = whisper_model.transcribe(segment_audio)\n",
        "                    transcript = \" \".join(s.text for s in segments_result).strip()\n",
        "\n",
        "                    if transcript:\n",
        "                        print(f\"[{turn['start']:.1f}s-{turn['end']:.1f}s] {turn['speaker']}: {transcript}\")\n",
        "\n",
        "            # Clean up temp file\n",
        "            try:\n",
        "                os.remove(audio_file)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        elif USE_SPEECHBRAIN and len(segments) > 1:\n",
        "            # Extract embeddings\n",
        "            print(\"Extracting speaker embeddings...\")\n",
        "            embeddings = extract_embeddings(segments)\n",
        "\n",
        "            if embeddings is not None and len(embeddings) > 1:\n",
        "                # Cluster embeddings - FIXED VERSION\n",
        "                num_speakers = min(2, len(embeddings))\n",
        "\n",
        "                # Calculate distance matrix with cosine distance\n",
        "                distance_matrix = pairwise_distances(embeddings, metric='cosine')\n",
        "\n",
        "                # Use AgglomerativeClustering without the 'affinity' parameter\n",
        "                clustering = AgglomerativeClustering(\n",
        "                    n_clusters=num_speakers,\n",
        "                    linkage=\"average\",\n",
        "                    # Remove the 'affinity' parameter\n",
        "                    # Use precomputed distances instead\n",
        "                    affinity='precomputed'\n",
        "                )\n",
        "                labels = clustering.fit_predict(distance_matrix)\n",
        "                print(f\"üîñ {len(set(labels))} speakers detected\")\n",
        "\n",
        "                # Transcribe segments with speaker labels\n",
        "                print(\"Transcribing with speaker labels...\")\n",
        "                for i, (start, end, segment_audio) in enumerate(segments):\n",
        "                    segment_audio = segment_audio.astype(np.float32)\n",
        "                    segments_result, _ = whisper_model.transcribe(segment_audio)\n",
        "                    transcript = \" \".join(s.text for s in segments_result).strip()\n",
        "\n",
        "                    if transcript:\n",
        "                        speaker_label = f\"Speaker {labels[i] + 1}\"\n",
        "                        start_time = start / SAMPLE_RATE\n",
        "                        end_time = end / SAMPLE_RATE\n",
        "                        print(f\"[{start_time:.1f}s-{end_time:.1f}s] {speaker_label}: {transcript}\")\n",
        "            else:\n",
        "                print(\"‚ö†Ô∏è Not enough segments for diarization\")\n",
        "                # Fall back to regular transcription\n",
        "                audio_data = audio_data.astype(np.float32)\n",
        "                segments_result, _ = whisper_model.transcribe(audio_data)\n",
        "                transcript = \" \".join(s.text for s in segments_result).strip()\n",
        "                if transcript:\n",
        "                    print(f\"üìù Transcript: {transcript}\")\n",
        "                else:\n",
        "                    print(\"‚ö†Ô∏è No speech detected\")\n",
        "        else:\n",
        "            # Just do regular transcription without diarization\n",
        "            print(\"Starting transcription (without speaker diarization)...\")\n",
        "            audio_data = audio_data.astype(np.float32)\n",
        "            segments_result, _ = whisper_model.transcribe(audio_data)\n",
        "            transcript = \" \".join(s.text for s in segments_result).strip()\n",
        "\n",
        "            if transcript:\n",
        "                print(f\"üìù Transcript: {transcript}\")\n",
        "            else:\n",
        "                print(\"‚ö†Ô∏è No speech detected\")\n",
        "\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        print(f\"‚ö†Ô∏è Error in process_audio: {str(e)}\")\n",
        "        print(traceback.format_exc())\n",
        "\n",
        "    print(\"‚úÖ Audio processing complete\")\n",
        "\n",
        "# Keyboard handling\n",
        "def on_press(key):\n",
        "    global RECORDING, recording_start_time, exit_flag\n",
        "    if key == keyboard.Key.esc:\n",
        "        exit_flag = True\n",
        "        return False  # Stop listener\n",
        "    elif key == keyboard.Key.space and not RECORDING:\n",
        "        RECORDING = True\n",
        "        recording_start_time = time.time()\n",
        "        audio_buffer.clear()\n",
        "        print(\"üé§ Recording started...\")\n",
        "\n",
        "def on_release(key):\n",
        "    global RECORDING, recording_start_time, audio_buffer, processing_thread\n",
        "    if key == keyboard.Key.space and RECORDING:\n",
        "        elapsed = time.time() - recording_start_time\n",
        "        if elapsed < MIN_RECORD_TIME:\n",
        "            print(f\"‚è≥ Hold space for at least {MIN_RECORD_TIME} sec to record.\")\n",
        "            return\n",
        "\n",
        "        RECORDING = False\n",
        "        print(\"üõë Recording stopped.\")\n",
        "\n",
        "        # Process the recorded audio\n",
        "        if audio_buffer:\n",
        "            # Concatenate the audio buffer\n",
        "            audio_data = np.concatenate(audio_buffer, axis=0).flatten()\n",
        "            audio_buffer = []\n",
        "\n",
        "            # Process in a separate thread\n",
        "            if processing_thread and processing_thread.is_alive():\n",
        "                print(\"‚ö†Ô∏è Previous processing still running, please wait...\")\n",
        "                processing_thread.join()\n",
        "\n",
        "            processing_thread = threading.Thread(target=process_audio_thread, args=(audio_data,))\n",
        "            processing_thread.daemon = True\n",
        "            processing_thread.start()\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è No audio recorded.\")\n",
        "\n",
        "# Cleanup and signal handling\n",
        "def cleanup():\n",
        "    print(\"\\nüõë Exiting... Cleaning up.\")\n",
        "    try:\n",
        "        if 'stream' in globals() and stream.active:\n",
        "            stream.close()\n",
        "        if processing_thread and processing_thread.is_alive():\n",
        "            processing_thread.join(timeout=1.0)  # Wait for processing to finish with timeout\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Cleanup error: {e}\")\n",
        "    print(\"‚úÖ Goodbye!\")\n",
        "\n",
        "def signal_handler(sig, frame):\n",
        "    global exit_flag\n",
        "    exit_flag = True\n",
        "    cleanup()\n",
        "    exit(0)\n",
        "\n",
        "signal.signal(signal.SIGINT, signal_handler)\n",
        "\n",
        "# Main\n",
        "if __name__ == \"__main__\":\n",
        "    # Set up audio stream\n",
        "    stream = sd.InputStream(\n",
        "        samplerate=SAMPLE_RATE,\n",
        "        channels=CHANNELS,\n",
        "        callback=callback,\n",
        "        dtype=np.float32,\n",
        "        device=DEVICE_INDEX,\n",
        "        blocksize=8000  # Smaller blocksize for more frequent callbacks\n",
        "    )\n",
        "\n",
        "    print(\"üéôÔ∏è Hold [SPACE] to record, release to process. Press [ESC] to exit.\")\n",
        "    print(\"üî¥ [CTRL+C] exits safely.\")\n",
        "\n",
        "    try:\n",
        "        stream.start()\n",
        "\n",
        "        # Use a simple approach with a listener that blocks\n",
        "        with keyboard.Listener(on_press=on_press, on_release=on_release) as listener:\n",
        "            # Keep checking if we should exit\n",
        "            while not exit_flag and listener.running:\n",
        "                time.sleep(0.1)\n",
        "\n",
        "        cleanup()\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Error in main: {e}\")\n",
        "        cleanup()\n",
        "\n"
      ]
    }
  ]
}